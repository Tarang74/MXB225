%!TEX TS-program = xelatex
%!TEX options = -aux-directory=Debug -shell-escape -file-line-error -interaction=nonstopmode -halt-on-error -synctex=1 "%DOC%"
\documentclass{article}
\input{LaTeX-Submodule/template.tex}

% Additional packages & macros

% Header and footer
\newcommand{\unitName}{Modelling with Differential Equations 1}
\newcommand{\unitTime}{Semester 1, 2023}
\newcommand{\unitCoordinator}{Prof Matthew Simpson}
\newcommand{\documentAuthors}{Tarang Janawalkar}

\fancyhead[L]{\unitName}
\fancyhead[R]{\leftmark}
\fancyfoot[C]{\thepage}

% Copyright
\usepackage[
    type={CC},
    modifier={by-nc-sa},
    version={4.0},
    imagewidth={5em},
    hyphenation={raggedright}
]{doclicense}

\date{}

\begin{document}
%
\begin{titlepage}
    \vspace*{\fill}
    \begin{center}
        \LARGE{\textbf{\unitName}} \\[0.1in]
        \normalsize{\unitTime} \\[0.2in]
        \normalsize\textit{\unitCoordinator} \\[0.2in]
        \documentAuthors
    \end{center}
    \vspace*{\fill}
    \doclicenseThis
    \thispagestyle{empty}
\end{titlepage}
\newpage
%
\tableofcontents
\newpage
%
\section{Differential Equations}
The study of differential equations (DE) finds techniques to solve equations relating unknown functions with their derivatives.
These equations allow us to model and predict the behaviour of systems evolving over time or any other dimension.
\subsection{Ordinary and Partial Differential Equations}
Differential equations fall into one of two categories:
\begin{itemize}
    \item \emph{Ordinary differential equations} (ODE) --- derivatives are taken with respect to only one variable.
    \item \emph{Partial differential equations} (PDE) --- derivatives are taken with respect to several variables.
\end{itemize}
Some examples of ODEs are shown below:
\begin{align*}
    \odv{y}{t}                                & = ky                    &  & (\text{Exponential growth})      \\
    \odv{y}{t}                                & = k\left( A - y \right) &  & (\text{Newton's law of cooling}) \\
    m \odv[order=2]{x}{t} + c \odv{x}{t} + kx & = f\left( t \right)     &  & (\text{Mechanical vibrations})
\end{align*}
where \(x\), \(y\), and \(f\) are functions of time - \(t\).
Below are some examples of PDEs:
\begin{align*}
    \pdv{u}{t} + c \pdv{u}{x} & = 0                    &  & (\text{Transport equation}) \\
    \pdv{u}{t}                & = k\pdv[order=2]{u}{x} &  & (\text{Heat equation})      \\
    \symbf{\Delta}u           & = 0                    &  & (\text{Laplace's equation})
\end{align*}
where \(u\) is typically a function of space \(\left( x,\: y,\: z \right)\) or space-time \(\left( x,\: y,\: z,\: t \right)\).
\subsection{Order}
The order of a differential equation refers to the highest derivative term that appears in the equation.
\subsection{Linearity}
A linear ODE is defined by a linear polynomial in the unknown function and its derivatives:
\begin{equation*}
    a_n\left( t \right) y^{\left( n \right)} + \cdots + a_2\left( t \right) y'' + a_1\left( t \right) y' + a_0\left( t \right) y = f\left( t \right).
\end{equation*}
Here the notation \(y^{\left( n \right)}\) denotes the \(n\)th derivative of \(y\), and we use this in-place of the apostrophe (\('\)) for compactness.
Note that the function \(y\) itself, can be thought of as the ``0''th derivative of \(y\).

Any ODE that cannot be expressed in the above form is nonlinear.

We can also classify PDEs in a similar manner. We will only consider a second-order linear PDE for brevity:
\begin{equation*}
    A\left( x,\: y \right) u_{xx} + B\left( x,\: y \right) u_{xy} + C\left( x,\: y \right) u_{yy} + D\left( x,\: y \right) u_x + E\left( x,\: y \right) u_y + F\left( x,\: y \right) u = f\left( x,\: y \right)
\end{equation*}
where we have a linear combination of second-order partial derivatives of \(u\).
Due to the complexity of PDEs, other classifications also exist, but these will not be discussed.
\subsection{Homogeneity}
Finally, a differential equation is homogeneous when it is an equation consisting only of the unknown function and its derivatives. In other words,
there are no constant terms (or functions).

Both ODEs and PDEs are classified by the same rules, below is an example of homogeneous DEs:
\begin{align*}
    a y'' + b y' + c y                         & = 0  \\
    \pdv[order=2]{u}{x} + k\pdv[order=2]{u}{t} & = 0.
\end{align*}
As we will see in further sections, differential equations are often simpler to solve when they are homogeneous.
\subsection{Useful Theorems}
\begin{theorem}[Existence and Uniqueness Theorem\footnote{Also known as the Picard-Lindel\"{o}f Theorem}]
    Let the functions \(f\) and \(\pdv{f}{y}\) be Lipschitz continuous\footnote{When \(f\) and \(\pdv{f}{y}\) are \emph{continuous}, then a solution exists with no information about its uniqueness. See the Peano Existence Theorem for more information.}
    on the closed rectangle \(D \subseteq \R\) with \(\left( t_0,\: y_0 \right) \in D\).
    Then there exists some interval \(I \subseteq D\), centred on \(t_0\) such that there is a unique solution to the initial value problem:
    \begin{equation*}
        \begin{cases}
            y'\left( t \right) = f\left( t,\: y \right) \\
            y\left( t_0 \right) = y_0
        \end{cases}
    \end{equation*}
\end{theorem}
\begin{corollary}[Existence and Uniqueness Theorem for Higher-Order ODEs]
    Let the functions \(\pdv[order=i]{f}{y}\) for all \(i = 0,\: 1,\: \dots,\: n - 1\), be Lipschitz continuous
    on the closed \(\left( n+1 \right)\)-dimensional hyperrectangle \(D \subseteq \R \times \R^n\) with \(\left( t_0,\: y_0,\: y_0',\: y_0'',\: \dots,\: y_0^{\left( n-1 \right)} \right) \in D\).
    Then there exists some interval \(I \subseteq D\) containing \(t_0\), such that there is a unique solution to the initial value problem:
    \begin{equation*}
        \begin{cases}
            y^{\left( n \right)} \left( t \right) = f\left( t,\: y,\: y',\: y'',\: \dots,\: y^{n-1}  \right) \\
            y^{\left( i \right)}\left( t_0 \right) = y_0^{\left( i \right)}
        \end{cases}
    \end{equation*}
    for all \(i = 0,\: 1,\: \dots,\: n - 1\). Note that \(y_0^{\left( i \right)}\) is the constant value associated with the initial value of the \(i\)th derivative of \(y\) at \(t_0\).
\end{corollary}
\begin{theorem}[Principle of Superposition]
    If \(y_1\) is a solution to the equation
    \begin{equation*}
        ay'' + by' + cy = f_1\left( t \right),
    \end{equation*}
    and \(y_2\) are solutions to
    \begin{equation*}
        ay'' + by' + cy = f_2\left( t \right),
    \end{equation*}
    then for any constants \(c_1\) and \(c_2\), the linear combination \(y = c_1 y_1 + c_2 y_2\) is a solution to the differential equation
    \begin{equation*}
        ay'' + by' + cy = c_1 f_1\left( t \right) + c_2 f_2\left( t \right).
    \end{equation*}
\end{theorem}
\begin{proof}
    We can prove the above result through substitution,
    \begin{align*}
        ay'' + by' + cy & = a\left( c_1 y_1 + c_2 y_2 \right)'' + b\left( c_1 y_1 + c_2 y_2 \right)' + c\left( c_1 y_1 + c_2 y_2 \right) \\
                        & = a c_1 y_1'' + a c_2 y_2'' + b c_1 y_1' + b c_2 y_2' + c c_1 y_1 + c c_2 y_2                                  \\
                        & = c_1 \left( a y_1'' + b y_1' + c y_1 \right) + c_2 \left( a y_2'' + b y_2' + c y_2 \right)                    \\
                        & = c_1 f_1\left( t \right) + c_2 f_2\left( t \right)
    \end{align*}
    A similar proof demonstrates that this principle also applies for higher-order linear ODEs.
\end{proof}
\section{Constant Coefficient ODEs}
A constant coefficient ODE is a linear ODE with coefficients that do not depend on the independent variable:
\begin{equation*}
    a_n y^{\left( n \right)} + \cdots + a_2 y'' + a_1 y' + a_0 y = f\left( t \right).
\end{equation*}
Using the superposition principle, we can express solutions as
\begin{equation*}
    y = c_1 y_h + c_2 y_p.
\end{equation*}
where:
\begin{itemize}
    \item \(y_h\) is a solution to the \emph{homogeneous} ODE, that is, when \(f\left( t \right) = 0\),
          and is known as the ``homogeneous solution''.
    \item \(y_p\) is a solution to the \emph{nonhomogeneous} ODE,
          and is known as the ``particular solution''.
\end{itemize}
\subsection{Homogeneous Solution}
As the homogeneous ODE is linear, we will assume\footnote{A common term for such assumptions is also \emph{ansatz}.} that the solution \(y_h\) has the form
\begin{equation*}
    y_h\left( t \right) = e^{r t}.
\end{equation*}
Let us then substitute this into the homogeneous ODE:
\begin{align*}
    a_n y_h^{\left( n \right)} + \cdots + a_2 y_h'' + a_1 y_h' + a_0 y_h     & = 0 \\
    a_n r^n e^{r t} + \cdots + a_2 r^2 e^{r t} + a_1 r e^{r t} + a_0 e^{r t} & = 0 \\
    \left( a_n r^n + \cdots + a_2 r^2 + a_1 r + a_0 \right) e^{r t}          & = 0 \\
    a_n r^n + \cdots + a_2 r^2 + a_1 r + a_0                                 & = 0
\end{align*}
This result is known as the \textbf{characterisic equation} (also \textbf{complementary equation} or \textbf{auxiliary equation}) of our ODE.
Finding the \emph{roots} of this algebraic equation allows us to find \(y_h\).
As this is a polynomial expression, we expect three kinds of solutions:
\begin{itemize}
    \item Real distinct roots: \(r_1,\: r_2,\: \dots,\: r_k \in \R\)
    \item Complex conjugate roots: \(r,\: r^\ast \in \C\)
    \item Real repeated roots: \(r \in \R\)
\end{itemize}
For higher order ODEs that may have real, repeated, and complex roots simultaneously, we can take the linear combination
of the solutions discussed below.
\subsubsection{Real Distinct Roots}
Given real distinct roots \(r_1,\: r_2,\: \dots,\: r_k\), the homogeneous solution is given by
\begin{equation*}
    y_h = c_1 e^{r_1 t} + c_2 e^{r_2 t} \cdots + c_k e^{r_k t}
\end{equation*}
for \(k \leq n\).
\subsubsection{Complex Conjugate Roots}
Given complex conjugate roots \(r = \alpha + \beta i\) and \(r^\ast = \alpha - \beta i\),
the homogeneous solution is given by
\begin{equation*}
    y_h = k_1 e^{\left( \alpha + \beta i \right) t} + k_2 e^{\left( \alpha - \beta i \right) t}.
\end{equation*}
We can then simplify this solution using Euler's identity.
\begin{align*}
    y_h & = e^{\alpha t} \left( k_1 e^{\beta i t} + k_2 e^{-\beta i t} \right)                                                                                                                            \\
        & = e^{\alpha t} \left[ k_1 \left( \cos{\left( \beta t \right)} + i\sin{\left( \beta t \right)} \right) + k_2 \left( \cos{\left( \beta t \right)} - i\sin{\left( \beta t \right)} \right) \right] \\
        & = e^{\alpha t} \left[ k_1 \cos{\left( \beta t \right)} + i k_1 \sin{\left( \beta t \right)} + k_2 \cos{\left( \beta t \right)} - i k_2 \sin{\left( \beta t \right)} \right]                     \\
        & = e^{\alpha t} \left[ \left( k_1 + k_2 \right) \cos{\left( \beta t \right)} + i \left( k_1 - k_2 \right) \sin{\left( \beta t \right)} \right]                                                   \\
        & = e^{\alpha t} \left[ c_1 \cos{\left( \beta t \right)} + c_2 \sin{\left( \beta t \right)} \right]
\end{align*}
which demonstrates that complex conjugate roots correspond to oscillatory solutions.
\subsubsection{Real Repeated Roots}
When \(r = -\frac{a_1}{2a_2}\) is a repeated root with multiplicity \(k\), a solution is given by
\begin{equation*}
    y_{h,1} = e^{rt}.
\end{equation*}
However, as we will see later, this is not sufficient.
\subsection{Reduction of Order}
The reduction of order method is a method for converting any linear ODE to another linear ODE of lower order.
If the solution \(y_1\) is known in advance, we can construct additional solutions via the following substitution:
\begin{equation*}
    y_2 = u\left( t \right) y_1.
\end{equation*}
\subsubsection{Obtaining the General Solution in a Repeated Roots Problem}
In the previous section, we were unable to find a complete set of homogeneous solutions to our ODE.
Therefore let us use this new method to find other solutions that satisfy the homogeneous equation.
\begin{equation*}
    y_{h,2} = u\left( t \right) y_{h,1}.
\end{equation*}
The following example uses a second-order ODE, but the process is applicable in general.
Before we can substitute this equation into the ODE, let us find its first and second derivatives:
\begin{gather*}
    y_{h,2}' = u' y_{h,1} + u y_{h,1}' \\
    y_{h,2}'' = u'' y_{h,1} + u' y_{h,1}' + u' y_{h,1}' + u y_{h,1}'' = u'' y_{h,1} + 2u' y_{h,1}' + u y_{h,1}''
\end{gather*}
We can now substitute this equation into the homogeneous ODE:
\begin{align*}
    a_2 y_{h,2}'' + a_1 y_{h,2}' + a_0 y_{h,2}                                                                               & = 0 \\
    a_2 \left( u'' y_{h,1} + 2u' y_{h,1}' + u y_{h,1}'' \right) + a_1 \left( u' y_{h,1} + u y_{h,1}' \right) + a_0 u y_{h,1} & = 0 \\
    a_2 u'' y_{h,1} + 2a_2 u' y_{h,1}' + a_2 u y_{h,1}'' + a_1 u' y_{h,1} + a_1 u y_{h,1}' + a_0 u y_{h,1}                   & = 0 \\
    a_2 u'' e^{rt} + 2a_2 u' r e^{rt} + a_2 u r^2 e^{rt} + a_1 u' e^{rt} + a_1 u r e^{rt} + a_0 u e^{rt}                     & = 0 \\
    \left[ a_2 u'' + 2a_2 r u' + a_2 r^2 u + a_1 u' + a_1 r u + a_0 u \right] e^{rt}                                         & = 0 \\
    a_2 u'' + \left( 2a_2 r + a_1 \right) u' + \left( a_2 r^2 + a_1 r + a_0 \right) u                                        & = 0
\end{align*}
As \(r = -\frac{a_1}{2a_2}\), the coefficients of both \(u'\) and \(u\) become zero, resulting in:
\begin{equation*}
    a_2 u'' = 0
\end{equation*}
so that \(u'' = 0\) because \(a_2 \neq 0\). Integrating twice gives the solution:
\begin{equation*}
    u\left( t \right) = k_1t + k_2.
\end{equation*}
Let \(k_2 = 0\) so that:
\begin{equation*}
    y_{h,2} = t e^{rt}.
\end{equation*}
Finally, combining both solutions yields the general homogeneous solution:
\begin{equation*}
    y = c_1 e^{rt} + c_2 t e^{rt}.
\end{equation*}
If follows that this result applies to higher-order linear ODEs with repeated roots of any multiplicity.
\begin{proposition}
    A linear homogeneous ODE of order \(n\) with the root \(r\) of multiplicity \(k \leq n\) has the fundamental form:
    \begin{equation*}
        y = e^{r t} \left( 1 + t + t^2 + \cdots + t^k \right).
    \end{equation*}
\end{proposition}
\subsection{Fundamental Set of Solutions}
When determining the solutions of an \(n\)th order linear homogeneous ODE, we require a \textbf{general} solution \(y_h\) which
consists of \(n\) linearly independent functions \(y_{h,i}\) (for \(i = 1,\: 2,\: \dots,\: n\)) such that the set of solutions
form a \textbf{fundamental set of solutions} over the problem domain.

As was alluded to before, this is done by taking the linear combination of each ``basis'' function \(y_{h,i}\),
to find the general homogeneous solution:
\begin{equation*}
    y_h = c_1 y_{h,1} + c_2 y_{h,2} + \cdots + c_n y_{h,n}.
\end{equation*}
This is

To show that a set of functions are linearly independent, we must construct a definitive test, therefore,
consider the following.
\subsubsection{Test for Linear Independence}
The set of \(\left( n-1 \right)\)-times differentiable functions \(S = \left\{ y_1,\: y_2,\: \dots,\: y_n \right\}\) is linearly independent iff
\begin{equation*}
    c_1 y_1 + c_2 y_2 + \cdots + c_n y_n = 0
\end{equation*}
has the trivial solution \(c_1 = \cdots = c_n = 0\), for all \(t\). Let us then differentiate this equation \(n - 1\) times to
find the following system of equations:
\begin{equation*}
    \begin{aligned}
        c_1 y_1 + c_2 y_2 + \cdots + c_n y_n                                                                & = 0             \\
        c_1 y_1' + c_2 y_2' + \cdots + c_n y_n'                                                             & = 0             \\
        c_1 y_1'' + c_2 y_2'' + \cdots + c_n y_n''                                                          & = 0             \\
                                                                                                            & \vdotswithin{=} \\
        c_1 y_1^{\left( n-1 \right)} + c_2 y_2^{\left( n-1 \right)} + \cdots + c_n y_n^{\left( n-1 \right)} & = 0
    \end{aligned}
\end{equation*}
which we can express as
\begin{equation*}
    \begin{bmatrix}
        y_1                      & y_2                      & \cdots & y_n                      \\
        y_1'                     & y_2'                     & \cdots & y_n'                     \\
        y_1''                    & y_2''                    & \cdots & y_n''                    \\
        \vdots                   & \vdots                   & \vdots & \vdots                   \\
        y_1^{\left( n-1 \right)} & y_2^{\left( n-1 \right)} & \cdots & y_n^{\left( n-1 \right)}
    \end{bmatrix}
    \begin{bmatrix}
        c_1    \\
        c_2    \\
        \vdots \\
        c_n
    \end{bmatrix}
    =
    \begin{bmatrix}
        0      \\
        0      \\
        \vdots \\
        0
    \end{bmatrix}.
\end{equation*}
Recall from Linear Algebra that the linear system:
\begin{equation*}
    \symbf{A} \symbfit{x} = \symbfit{b}
\end{equation*}
has a unique solution when \(\vrank{\left( \symbf{A} \right)} =
\vrank{\left( \begin{bmatrix}[c|c]
        \symbf{A} & \symbfit{b}
    \end{bmatrix} \right)} = n\), where \(\symbfit{b} = \symbf{0}\). This is true when the matrix
\(\symbf{A}\) is invertible, so that
\begin{align*}
    \symbf{A} \symbfit{c}                & = \symbf{0}                \\
    \symbf{A}^{-1} \symbf{A} \symbfit{c} & = \symbf{A}^{-1} \symbf{0} \\
    \symbfit{c}                          & = \symbf{0}
\end{align*}
as required.

Therefore we can conclude that when the determinant of \(\symbf{A}\) is nonzero, the set \(S\) is linearly independent.
Let us summarise these findings in the following section and introduce the Wronskian matrix.
\subsubsection{The Wronskian}
\begin{definition}[Wronskian Matrix]
    Given the set of \(\left( n-1 \right)\)-times differentiable functions\linebreak \(S = \left\{ y_1,\: y_2,\: \dots,\: y_n \right\}\),
    the Wronskian matrix \(\symbf{W}\) is defined as
    \begin{equation*}
        \symbf{W} = \begin{bmatrix}
            y_1                      & y_2                      & \cdots & y_n                      \\
            y_1'                     & y_2'                     & \cdots & y_n'                     \\
            y_1''                    & y_2''                    & \cdots & y_n''                    \\
            \vdots                   & \vdots                   & \vdots & \vdots                   \\
            y_1^{\left( n-1 \right)} & y_2^{\left( n-1 \right)} & \cdots & y_n^{\left( n-1 \right)}
        \end{bmatrix}.
    \end{equation*}
\end{definition}
The ``Wronskian'' of these functions is given by
\begin{equation*}
    W = W\left[ y_1,\: y_2,\: \dots,\: y_n  \right] = \det{\left( \symbf{W} \right)}.
\end{equation*}
\begin{theorem}[Linear Independence and the Wronskian]
    The set of \(\left( n-1 \right)\)-times differentiable functions \(S = \left\{ y_1,\: y_2,\: \dots,\: y_n \right\}\)
    is linearly independent if
    \begin{equation*}
        W\left[ y_1,\: y_2,\: \dots,\: y_n  \right] \neq 0.
    \end{equation*}
    Note that the converse does not apply:
    \(W\left[ y_1,\: y_2,\: \dots,\: y_n  \right] = 0\) does not imply that \(S\) is linearly dependent.
\end{theorem}
\begin{corollary}
    If the set \(S\) is linearly dependent, then
    \begin{equation*}
        W\left[ y_1,\: y_2,\: \dots,\: y_n  \right] = 0.
    \end{equation*}
\end{corollary}
\subsection{Particular Solution}
As with the homogeneous solution, the method of \emph{Undetermined Coefficients}
requires us to guess a solution \(y_p\) based on the form of \(f\left( t \right)\)\footnote{\(f\left( t \right)\) is known as the
    \textbf{forcing function} or \textbf{input} in the system, and only depends on \(t\).}.
\subsubsection{Choosing a Particular Solution}
Consider the vector space of particularly nice functions \(W\) with the basis:
\begin{equation*}
    \vbasis{\left( W \right)} = \left\{ 1,\: t^n,\: e^{r t},\: e^{-r t},\: \sin{\left( r t \right)}, \cos{\left( r t \right)} \right\}
\end{equation*}
for all \(n \in \N_{>0}\) and \(r \in \R_{>0}\).

If \(f\) can be written as a linear combination or product of functions in \(W\),
then we can construct a particular solution \(y_p\) that satisfies the nonhomogeneous ODE.
The table below shows the choice of \(y_p\) for various \(f\).
\begin{table}[H]
    \centering
    \begin{tabular}{c | c}
        \toprule
        \textbf{Form of \(f\)}                                   & \textbf{Particular Solution \(y_p\)}                                                                                                                                               \\
        \midrule
        \(P_n\left( t \right)\)                                  & \(\displaystyle \sum_{i = 0}^n A_i t^i\)                                                                                                                                           \\
        \(e^{r t}\)                                              & \(C e^{r t}\)                                                                                                                                                                      \\
        \(T\left( \omega t \right)\)                             & \(K \cos{\left( \omega t \right)} + M \sin{\left( \omega t \right)}\)                                                                                                              \\
        \midrule
        \(P_n\left( t \right) e^{r t}\)                          & \(\displaystyle \left( \sum_{i = 0}^n A_i t^i \right) e^{r t}\)                                                                                                                    \\
        \(e^{r t} T\left( \omega t \right)\)                     & \(\displaystyle e^{r t} \left[ K \cos{\left( \omega t \right)} + M \sin{\left( \omega t \right)} \right]\)                                                                         \\
        \(P_n\left( t \right) e^{r t} T\left( \omega t \right)\) & \(\displaystyle e^{r t} \left[ \left( \sum_{i = 0}^n K_i t^i \right) \cos{\left( \omega t \right)} + \left( \sum_{i = 0}^n M_i t^i \right) \sin{\left( \omega t \right)} \right]\) \\
        \bottomrule
    \end{tabular}
    \caption{Choice of particular solution based on the form of the nonhomogeneous term where
        \(n \in \N_{\geq 0}\), \(r,\: \omega \in \R_{>0}\), and \(A,\: C,\: K,\: M \in \R\).
        Any constant coefficients in \(f\) are to be ignored.} % \label{}
\end{table}
In the above table, \(P_n\left( t \right)\) is any polynomial function of order \(n\),
\(T\left( t \right)\) is used to denote any of the following trigonometric functions:
\begin{equation*}
    T\left( t \right) =
    \begin{cases}
        \cos{\left( t \right)} \\
        \sin{\left( t \right)} \\
        \cos{\left( t \right)} \pm \sin{\left( t \right)}
    \end{cases}
\end{equation*}
For more complex forms of \(f\) as shown in the last three rows of the table above, we can use the following facts:
\begin{itemize}
    \item If \(f\) can be expressed as the linear combination of two forms \(f_1\) and \(f_2\), then the particular solution
          is also a linear combination of the particular solutions associated with \(f_1\) and \(f_2\).
    \item If \(f\) can be expressed as a product of two forms \(f_1\) and \(f_2\), then the particular solution will also be
          the product of the particular solutions associated with \(f_1\) and \(f_2\).
          In this situation, it is important to always simplify all arbitrary constants as shown in the table above.
\end{itemize}
In general, it is sufficient to only memorise the first three rows of the table above.
\subsubsection{Undetermined Coefficients}
Equipped with a particular solution \(y_p\), we can substitute it into the LHS of the nonhomogeneous ODE
to obtain:
\begin{equation*}
    a_n y_p^{\left( n \right)} + \cdots + a_2 y_p'' + a_1 y_p' + a_0 y_p = f\left( t \right).
\end{equation*}
We then simplify the LHS and factor out any terms that are terms that depend on \(t\).
For example:
\begin{equation*}
    \left( \dots \right) f_1 + \left( \dots \right) f_2 + \cdots + \left( \dots \right) f_3 f_4 + \cdots = f.
\end{equation*}
The purpose of doing so is to construct a system of equations that allows us to solve for the undetermined coefficients in \(y_p\).
\subsubsection{The Resonance Case}
In some problems, the forcing term \(f\left( t \right)\) may have a form similar to
\(y_h\) leading to a linearly dependent solution \(y_p\). In this case, we can use the \emph{Reduction of Order} method, as we did for
repeated roots, and multiply \(y_p\) by \(t\) until it is linearly independent to \(y_h\).
\subsection{Initial Value Problems}
Real-world systems modelled using differential equations are accompanied by initial conditions
that allow us to eliminate any remaining constants in our general solution \emph{after}
we have found \textbf{both} the homogeneous and particular solutions (where applicable).

For an \(n\)th order ODE, we require \(n\) initial conditions \(y_0\), corresponding to the first \(n\) derivatives of \(y\):
\begin{align*}
    y\left( t_0 \right)                      & = y_0                      \\
    y'\left( t_0 \right)                     & = y_0'                     \\
                                             & \vdotswithin{=}            \\
    y^{\left( n-1 \right)}\left( t_0 \right) & = y_0^{\left( n-1 \right)}
\end{align*}
note that \(y_0\) is a constant and we are using the dash (\('\)) for notational convenience.
\section{Nonconstant Coefficient ODEs}
A nonconstant coefficient ODE (or a linear ODE) has the following form:
\begin{equation*}
    a_n\left( t \right) y^{\left( n \right)} + \cdots + a_2\left( t \right) y'' + a_1\left( t \right) y' + a_0\left( t \right) y = f\left( t \right).
\end{equation*}
where \(a_i\) depends on the independent variable.

In the previous section, we found that the method of Undetermined Coefficients was insufficient for ODEs with more complex
forcing functions. As we will see soon, we can construct a different particular solution where our guess depends on the
homogeneous solutions.
\subsection{Variation of Parameters}
Given a set of homogeneous solutions \(y_1\), \(y_2\), \dots, and \(y_n\) to the above linear ODE,
consider a particular solution of the form
\begin{equation*}
    y_p = v_1\left( t \right) y_1 + v_2\left( t \right) y_2 + \cdots + v_n\left( t \right) y_n
\end{equation*}
where \(v_i\) are now functions of the independent variable.

As before, we want to substitute this function into the linear ODE above, which requires us
to compute the derivatives of \(y_p\). Because \(y_p\) has several products, we will impose
additional constraints such that we eliminate any derivatives of \(v\) in our calculations (except for in the final derivative)\footnote{See the appendix for an example using a second-order linear ODE.}.

This will give:
\begin{align*}
    y_p^{\left( k \right)} & = v_1 y_1^{\left( k \right)} + v_2 y_2^{\left( k \right)} + \cdots + v_n y_n^{\left( k \right)}                                                                                                          &  & k = 0,\: 1,\: \dots,\: n - 1 \\
    y_p^{\left( n \right)} & = v_1 y_1^{\left( n \right)} + v_2 y_2^{\left( n \right)} + \cdots + v_n y_n^{\left( n \right)} + v_1' y_1^{\left( n-1 \right)} + v_2' y_2^{\left( n-1 \right)} + \cdots + v_n' y_n^{\left( n-1 \right)}
\end{align*}
where we make the following constraints:
\begin{align*}
    0 & = v_1' y_1^{\left( k \right)} + v_2' y_2^{\left( k \right)} + \cdots + v_n' y_n^{\left( k \right)} &  & k = 0,\: 1,\: \dots,\: n - 2.
\end{align*}
Explicitly this looks like the following:
\begin{align*}
    \begin{cases}
        y_p                        = v_1 y_1 + v_2 y_2 + \cdots + v_n y_n                                                                      \\
        y_p'                       = v_1 y_1' + v_2 y_2' + \cdots + v_n y_n'                                                                   \\
        \vdots                                                                                                                                 \\
        y_p^{\left( n - 1 \right)} = v_1 y_1^{\left( n - 1 \right)} + v_2 y_2^{\left( n - 1 \right)} + \cdots + v_n y_n^{\left( n - 1 \right)} \\
        y_p^{\left( n \right)} = v_1 y_1^{\left( n \right)} + v_2 y_2^{\left( n \right)} + \cdots + v_n y_n^{\left( n \right)} + v_1' y_1^{\left( n-1 \right)} + v_2' y_2^{\left( n-1 \right)} + \cdots + v_n' y_n^{\left( n-1 \right)}
    \end{cases}
\end{align*}
with the constraints:
\begin{equation*}
    \begin{cases}
        0 = v_1' y_1 + v_2' y_2 + \cdots + v_n' y_n    \\
        0 = v_1' y_1' + v_2' y_2' + \cdots + v_n' y_n' \\
        \vdots                                         \\
        0 = v_1' y_1^{\left( n-2 \right)} + v_2' y_2^{\left( n-2 \right)} + \cdots + v_n' y_n^{\left( n-2 \right)}
    \end{cases}
\end{equation*}
We can now substitute \(y_p\) into the ODE to find
\begin{align*}
    a_n y_p^{\left( n \right)} + a_{n-1} y_p^{\left( n-1 \right)} + \cdots + a_1 y_p' + a_0 y_p                                     & = f\left( t \right) \\[2ex]
    \begin{aligned}[b]
        a_0 \left[ v_1 y_1 + v_2 y_2 + \cdots + v_n y_n \right] +                                                          & \\
        a_1 \left[ v_1 y_1' + v_2 y_2' + \cdots + v_n y_n' \right] +                                                       & \\
        \mathmakebox[\widthof{\(+\)}]{\vdots}                                                                              & \\
        a_n \left[ v_1 y_1^{\left( n \right)} + v_2 y_2^{\left( n \right)} + \cdots + v_n y_n^{\left( n \right)} \right] + & \\
        a_n \left[ v_1' y_1^{\left( n-1 \right)} + v_2' y_2^{\left( n-1 \right)} + \cdots + v_n' y_n^{\left( n-1 \right)} \right]
    \end{aligned} & = f\left( t \right)                          \\[2ex]
    \begin{aligned}[b]
        v_1 \left[ a_n y_1^{\left( n \right)} + a_{n-1} y_1^{\left( n - 1 \right)} + \cdots + a_1 y_1' + a_0 y_1 \right] + & \\
        v_2 \left[ a_n y_2^{\left( n \right)} + a_{n-1} y_2^{\left( n - 1 \right)} + \cdots + a_1 y_2' + a_0 y_2 \right] + & \\
        \mathmakebox[\widthof{\(+\)}]{\vdots}                                                                              & \\
        v_n \left[ a_n y_n^{\left( n \right)} + a_{n-1} y_n^{\left( n - 1 \right)} + \cdots + a_1 y_n' + a_0 y_n \right] + & \\
        a_n \left[ v_1' y_1^{\left( n-1 \right)} + v_2' y_2^{\left( n-1 \right)} + \cdots + v_n' y_n^{\left( n-1 \right)} \right]
    \end{aligned} & = f\left( t \right)
\end{align*}
This simplifies greatly and results in:
\begin{equation*}
    v_1' y_1^{\left( n-1 \right)} + v_2' y_2^{\left( n-1 \right)} + \cdots + v_n' y_n^{\left( n-1 \right)} = g\left( t \right)
\end{equation*}
where \(g\left( t \right) = f\left( t \right) / a_n\). Using the constraints stated previously, we reach the following
system of equations:
\begin{equation*}
    \begin{bmatrix}
        y_1                      & y_2                      & \cdots & y_n                      \\
        y_1'                     & y_2'                     & \cdots & y_n'                     \\
        \vdots                   & \vdots                   & \vdots & \vdots                   \\
        y_1^{\left( n-2 \right)} & y_2^{\left( n-2 \right)} & \cdots & y_n^{\left( n-2 \right)} \\
        y_1^{\left( n-1 \right)} & y_2^{\left( n-1 \right)} & \cdots & y_n^{\left( n-1 \right)}
    \end{bmatrix}
    \begin{bmatrix}
        v_1'     \\
        v_2'     \\
        \vdots   \\
        v_n'
    \end{bmatrix}
    =
    \begin{bmatrix}
        0      \\
        0      \\
        \vdots \\
        0      \\
        g\left( t \right)
    \end{bmatrix}.
\end{equation*}
Using Cramer's rule, we find that the solution to this system is given by
\begin{equation*}
    v_i' = \frac{W_i}{W}
\end{equation*}
where \(W\) is the Wronskian, and \(W_i\) is the determinant of the Wronskian matrix where the \(i\)th column is replaced with the RHS vector \(\symbfit{b}\).
Integration shows that the particular solution is given by:
\begin{equation*}
    y_p = y_1 \int \frac{W_1}{W} \odif{t} + y_2 \int \frac{W_2}{W} \odif{t} + \cdots + y_n \int \frac{W_n}{W} \odif{t}.
\end{equation*}
Therefore the general solution of a linear ODE has the form:
\begin{equation*}
    y = y_1 + y_2 + \cdots + y_n + y_1 \int \frac{W_1}{W} \odif{t} + y_2 \int \frac{W_2}{W} \odif{t} + \cdots + y_n \int \frac{W_n}{W} \odif{t}
\end{equation*}
where \(y_1\) through \(y_n\) are fundamental solutions to the homogeneous ODE.
As we made no assumptions about the coefficients in this ODE, Variation of Parameters can be used to solve both constant coefficient and nonconstant coefficient ODEs.
It should be noted that the final integration may not have a closed-form solution.
\newpage
\begin{appendix}
    \section{Second-Order Variation of Parameters}
    Consider the following second-order linear ODE
    \begin{equation*}
        a_2\left( t \right) y'' + a_1\left( t \right) y' + a_0\left( t \right) y = f\left( t \right).
    \end{equation*}
    The particular solution is given by
    \begin{equation*}
        y_p = v_1 \left( t \right) y_1 + v_2\left( t \right) y_2
    \end{equation*}
    where \(y_1\) and \(y_2\) are solutions to the homogeneous problem.
    Differentiating \(y_p\) once gives us
    \begin{align*}
        y_p' = v_1' y_1 + v_1 y_1' + v_2' y_2 + v_2 y_2'
    \end{align*}
    where we will use the assumption
    \begin{equation*}
        v_1' y_1 + v_2' y_2 = 0
    \end{equation*}
    to arrive at
    \begin{equation*}
        y_p' = v_1 y_1' + v_2 y_2'.
    \end{equation*}
    The second derivative is then
    \begin{equation*}
        y_p'' = v_1' y_1' + v_1 y_1'' + v_2' y_2' + v_2 y_2''.
    \end{equation*}
    If we then substitute these values into our ODE, and factor in terms of \(v\), we find
    \begin{align*}
        a_2 y_p'' + a_1 y_p' + a_0 y_p                                                                                                                   & = f\left( t \right) \\
        a_2 \left[ v_1' y_1' + v_1 y_1'' + v_2' y_2' + v_2 y_2'' \right] + a_1 \left[ v_1 y_1' + v_2 y_2' \right] + a_0 \left[ v_1 y_1 + v_2 y_2 \right] & = f\left( t \right) \\
        v_1 \left[ a_2 y_1'' + a_1 y_1' + a_0 y_1 \right] + v_2 \left[ a_2 y_2'' + a_1 y_2' + a_0 y_2 \right] + a_2 v_1' y_1' + a_2 v_2' y_2'            & = f\left( t \right) \\
        a_2 v_1' y_1' + a_2 v_2' y_2'                                                                                                                    & = f\left( t \right) \\
        v_1' y_1' + v_2' y_2'                                                                                                                            & = g\left( t \right)
    \end{align*}
    where \(g\left( t \right) = f\left( t \right) / a_2\left( t \right)\).
    This gives us a system of equations where we can solve for \(v_1'\) and \(v_2'\),
    using the assumption from earlier:
    \begin{equation*}
        \begin{bmatrix}
            y_1  & y_2  \\
            y_1' & y_2'
        \end{bmatrix}
        \begin{bmatrix}
            v_1' \\
            v_2'
        \end{bmatrix}
        =
        \begin{bmatrix}
            0 \\
            g\left( t \right)
        \end{bmatrix}.
    \end{equation*}
    where the coefficient matrix is simply the Wronskian matrix \(\symbf{W}\).
    Using Cramer's rule, we can conclude that
    \begin{equation*}
        v_1' = \frac{W_1}{W} \qquad \text{and} \qquad v_2' = \frac{W_2}{W}
    \end{equation*}
    where \(W_i\) are the determinants of the Wronskian with the \(i\)th columns replaced with the RHS vector \(\symbfit{b}\):
    \begin{equation*}
        W_1 = \begin{vmatrix}
            0                 & y_2  \\
            g\left( t \right) & y_2'
        \end{vmatrix}
        \qquad \text{and} \qquad
        W_2 = \begin{vmatrix}
            y_1  & 0                 \\
            y_1' & g\left( t \right)
        \end{vmatrix}
    \end{equation*}
    Integrating these results gives
    \begin{equation*}
        v_1 = \int \frac{W_1}{W} \odif{t} \qquad \text{and} \qquad v_2 = \int \frac{W_2}{W} \odif{t}
    \end{equation*}
    so that the particular solution is given by
    \begin{equation*}
        y_p = y_1 \int \frac{W_1}{W} \odif{t} + y_2 \int \frac{W_2}{W} \odif{t}.
    \end{equation*}
    Note that it may not be possible to compute these integrals.
\end{appendix}
\end{document}
